{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Deferral Score for WCP-L2D\n",
    "\n",
    "Binary CP with RAPS produces discrete prediction sets ({0}, {1}, or {0,1}),\n",
    "causing all-or-nothing deferral behavior. This notebook replaces set-size\n",
    "thresholding with a **continuous uncertainty score** for deferral.\n",
    "\n",
    "**Approach**: Use $u(x) = 1 - \\max(\\text{softmax}(x))$ as a continuous\n",
    "uncertainty measure. Calibrate the deferral threshold using DRE-weighted\n",
    "calibration scores to account for covariate shift.\n",
    "\n",
    "This avoids the binary CP bottleneck and produces smooth accuracy-rejection\n",
    "curves with tunable operating points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.special import softmax\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from wcp_l2d.features import ExtractedFeatures\n",
    "from wcp_l2d.pathologies import COMMON_PATHOLOGIES\n",
    "from wcp_l2d.label_utils import extract_binary_labels\n",
    "from wcp_l2d.dre import AdaptiveDRE\n",
    "from wcp_l2d.conformal import ConformalPredictor, WeightedConformalPredictor\n",
    "from wcp_l2d.evaluation import (\n",
    "    compute_system_accuracy,\n",
    "    _predictions_from_sets,\n",
    "    compute_coverage,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "EXPERT_ACCURACY = 0.85\n",
    "FEATURE_DIR = Path(\"../data/features\")\n",
    "TARGET_PATHOLOGY = \"Effusion\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "chexpert = ExtractedFeatures.load(\n",
    "    FEATURE_DIR / \"chexpert_densenet121-res224-chex_features.npz\"\n",
    ")\n",
    "nih = ExtractedFeatures.load(FEATURE_DIR / \"nih_densenet121-res224-chex_features.npz\")\n",
    "\n",
    "# Binary labels for target pathology\n",
    "chex_feats, chex_labels, _ = extract_binary_labels(\n",
    "    chexpert.features, chexpert.labels, COMMON_PATHOLOGIES, TARGET_PATHOLOGY\n",
    ")\n",
    "nih_feats, nih_labels, _ = extract_binary_labels(\n",
    "    nih.features, nih.labels, COMMON_PATHOLOGIES, TARGET_PATHOLOGY\n",
    ")\n",
    "\n",
    "# Splits\n",
    "chex_train_feats, chex_temp_feats, chex_train_labels, chex_temp_labels = (\n",
    "    train_test_split(\n",
    "        chex_feats, chex_labels, test_size=0.4, random_state=SEED, stratify=chex_labels\n",
    "    )\n",
    ")\n",
    "chex_cal_feats, chex_test_feats, chex_cal_labels, chex_test_labels = train_test_split(\n",
    "    chex_temp_feats,\n",
    "    chex_temp_labels,\n",
    "    test_size=0.5,\n",
    "    random_state=SEED,\n",
    "    stratify=chex_temp_labels,\n",
    ")\n",
    "\n",
    "# NIH pool (DRE) + test\n",
    "rng = np.random.RandomState(SEED)\n",
    "nih_all_perm = rng.permutation(len(nih.features))\n",
    "nih_pool_feats_all = nih.features[nih_all_perm[: len(nih.features) // 2]]\n",
    "\n",
    "_, nih_test_feats, _, nih_test_labels = train_test_split(\n",
    "    nih_feats, nih_labels, test_size=0.5, random_state=SEED, stratify=nih_labels\n",
    ")\n",
    "\n",
    "# Classifier\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(chex_train_feats)\n",
    "X_cal = scaler.transform(chex_cal_feats)\n",
    "X_test_nih = scaler.transform(nih_test_feats)\n",
    "\n",
    "clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, C=1.0, random_state=SEED)\n",
    "clf.fit(X_train, chex_train_labels)\n",
    "\n",
    "\n",
    "def get_binary_logits(clf, X):\n",
    "    d = clf.decision_function(X)\n",
    "    return np.column_stack([-d, d])\n",
    "\n",
    "\n",
    "cal_logits = get_binary_logits(clf, X_cal)\n",
    "test_nih_logits = get_binary_logits(clf, X_test_nih)\n",
    "\n",
    "# DRE\n",
    "dre = AdaptiveDRE(n_components=4, weight_clip=20.0, random_state=SEED)\n",
    "dre.fit(chex_cal_feats, nih_pool_feats_all)\n",
    "cal_weights = dre.compute_weights(chex_cal_feats)\n",
    "test_nih_weights = dre.compute_weights(nih_test_feats)\n",
    "\n",
    "print(f\"CheXpert cal: {len(chex_cal_labels)} (prev={chex_cal_labels.mean():.3f})\")\n",
    "print(f\"NIH test:     {len(nih_test_labels)} (prev={nih_test_labels.mean():.3f})\")\n",
    "print(f\"NIH AUC: {roc_auc_score(nih_test_labels, clf.predict_proba(X_test_nih)[:, 1]):.4f}\")\n",
    "print(f\"DRE ESS: {dre.diagnostics(chex_cal_feats).ess_fraction:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continuous Deferral Methods\n",
    "\n",
    "**Uncertainty score**: $u(x) = 1 - \\max_y \\text{softmax}(f(x))_y$\n",
    "\n",
    "**Deferral rule**: defer if $u(x) > \\tau$\n",
    "\n",
    "**Threshold calibration**:\n",
    "- *Uncalibrated*: sweep $\\tau$ directly (Max Logit baseline)\n",
    "- *Source-calibrated*: $\\tau = Q_{1-\\alpha}(\\{u_i\\}_{i=1}^{n_{\\text{cal}}})$ &mdash; source quantile\n",
    "- *Weighted (WCD)*: $\\tau = Q^w_{1-\\alpha}(\\{u_i\\}_{i=1}^{n_{\\text{cal}}})$ &mdash; DRE-weighted quantile,\n",
    "  calibrated for the target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import math\n\ndef compute_uncertainty(logits):\n    \"\"\"u(x) = 1 - max(softmax(logits)). Higher = more uncertain.\"\"\"\n    probs = softmax(logits, axis=1)\n    return 1 - probs.max(axis=1)\n\n\ndef weighted_quantile(scores, level, weights=None):\n    \"\"\"Compute (weighted) quantile of scores at given level.\"\"\"\n    sort_idx = np.argsort(scores)\n    sorted_s = scores[sort_idx]\n\n    if weights is not None:\n        sorted_w = weights[sort_idx]\n        cumprob = np.cumsum(sorted_w) / sorted_w.sum()\n    else:\n        n = len(sorted_s)\n        cumprob = np.arange(1, n + 1) / n\n\n    idx = np.searchsorted(cumprob, level)\n    if idx >= len(sorted_s):\n        return float(\"inf\")\n    return float(sorted_s[idx])\n\n\ndef accuracy_rejection_curve(\n    test_uncertainty, test_logits, test_labels, expert_accuracy=0.85, n_points=200\n):\n    \"\"\"Compute ARC by sweeping threshold over test uncertainty scores.\"\"\"\n    model_preds = np.argmax(test_logits, axis=1)\n    thresholds = np.percentile(\n        test_uncertainty, np.linspace(0, 100, n_points + 1)[:-1]\n    )\n    thresholds = np.append(thresholds, test_uncertainty.max() + 1)\n\n    deferral_rates = []\n    system_accs = []\n    model_accs_kept = []\n\n    for tau in thresholds:\n        defer_mask = test_uncertainty > tau\n        m = compute_system_accuracy(\n            model_preds, test_labels, defer_mask, expert_accuracy=expert_accuracy\n        )\n        deferral_rates.append(m[\"deferral_rate\"])\n        system_accs.append(m[\"system_accuracy\"])\n        model_accs_kept.append(m[\"model_accuracy_on_kept\"])\n\n    return np.array(deferral_rates), np.array(system_accs), np.array(model_accs_kept)\n\n\ndef cp_deferral_curve(cal_logits, cal_labels, test_logits, test_labels,\n                      cal_weights=None, test_weights=None,\n                      expert_accuracy=0.85, n_alphas=30):\n    \"\"\"Compute ARC for CP/WCP. Optimized: scores computed once, alpha swept.\"\"\"\n    from torchcp.classification.score import RAPS as _RAPS\n\n    alphas = np.linspace(0.01, 0.99, n_alphas)\n    score_fn = _RAPS(penalty=0.1, kreg=1, randomized=False)\n\n    # Compute RAPS scores ONCE\n    cal_logits_t = torch.tensor(cal_logits, dtype=torch.float32)\n    cal_labels_t = torch.tensor(cal_labels, dtype=torch.long)\n    test_logits_t = torch.tensor(test_logits, dtype=torch.float32)\n\n    cal_scores = score_fn(cal_logits_t, cal_labels_t).numpy()\n    test_scores = score_fn(test_logits_t).numpy()  # [N_test, K]\n\n    deferral_rates = []\n    system_accs = []\n    model_accs_kept = []\n\n    if cal_weights is not None:\n        # WCP: precompute cumprob ONCE, sweep alpha threshold\n        sort_idx = np.argsort(cal_scores)\n        cal_scores_sorted = cal_scores[sort_idx]\n        cal_w_sorted = cal_weights[sort_idx]\n\n        n_cal = len(cal_scores_sorted)\n        N_test = len(test_weights)\n\n        # Build weight matrix and normalize [N_test, n_cal+1]\n        cal_w_row = cal_w_sorted[np.newaxis, :]  # [1, n_cal]\n        test_w_col = test_weights[:, np.newaxis]  # [N_test, 1]\n        all_w = np.concatenate(\n            [np.broadcast_to(cal_w_row, (N_test, n_cal)), test_w_col], axis=1\n        )\n        p = all_w / all_w.sum(axis=1, keepdims=True)\n        cumprob = np.cumsum(p[:, :n_cal], axis=1)  # [N_test, n_cal]\n\n        for alpha in alphas:\n            target = 1 - alpha\n            reached = cumprob >= target\n            has_any = reached.any(axis=1)\n            first_idx = np.argmax(reached, axis=1)\n            q_hat = np.where(has_any, cal_scores_sorted[first_idx], np.inf)\n            ps = (test_scores <= q_hat[:, np.newaxis]).astype(np.int32)\n\n            preds, defer_mask = _predictions_from_sets(ps, test_logits)\n            m = compute_system_accuracy(\n                preds, test_labels, defer_mask, expert_accuracy=expert_accuracy\n            )\n            deferral_rates.append(m[\"deferral_rate\"])\n            system_accs.append(m[\"system_accuracy\"])\n            model_accs_kept.append(m[\"model_accuracy_on_kept\"])\n    else:\n        # Standard CP: just sweep quantile index\n        sorted_scores = np.sort(cal_scores)\n        n = len(sorted_scores)\n\n        for alpha in alphas:\n            k = math.ceil((n + 1) * (1 - alpha))\n            q_hat = float(sorted_scores[k - 1]) if k <= n else float(\"inf\")\n            ps = (test_scores <= q_hat).astype(np.int32)\n\n            preds, defer_mask = _predictions_from_sets(ps, test_logits)\n            m = compute_system_accuracy(\n                preds, test_labels, defer_mask, expert_accuracy=expert_accuracy\n            )\n            deferral_rates.append(m[\"deferral_rate\"])\n            system_accs.append(m[\"system_accuracy\"])\n            model_accs_kept.append(m[\"model_accuracy_on_kept\"])\n\n    return np.array(deferral_rates), np.array(system_accs), np.array(model_accs_kept)\n\n\n# Pre-compute uncertainty scores\ncal_u = compute_uncertainty(cal_logits)\ntest_u = compute_uncertainty(test_nih_logits)\n\nprint(f\"Cal uncertainty:  mean={cal_u.mean():.4f}  std={cal_u.std():.4f}  \"\n      f\"median={np.median(cal_u):.4f}\")\nprint(f\"Test uncertainty: mean={test_u.mean():.4f}  std={test_u.std():.4f}  \"\n      f\"median={np.median(test_u):.4f}\")\nprint(\"Methods defined.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accuracy-Rejection Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ARCs for all methods\n",
    "\n",
    "# Continuous: sweep threshold over test uncertainty\n",
    "dr_cont, sa_cont, ma_cont = accuracy_rejection_curve(\n",
    "    test_u, test_nih_logits, nih_test_labels\n",
    ")\n",
    "\n",
    "# CP-based: Standard CP and WCP\n",
    "dr_cp, sa_cp, ma_cp = cp_deferral_curve(\n",
    "    cal_logits, chex_cal_labels, test_nih_logits, nih_test_labels\n",
    ")\n",
    "dr_wcp, sa_wcp, ma_wcp = cp_deferral_curve(\n",
    "    cal_logits, chex_cal_labels, test_nih_logits, nih_test_labels,\n",
    "    cal_weights=cal_weights, test_weights=test_nih_weights\n",
    ")\n",
    "\n",
    "print(\"ARCs computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# System accuracy vs deferral\n",
    "ax = axes[0]\n",
    "ax.plot(dr_cont, sa_cont, label=\"Continuous (sweep)\", color=\"#2ca02c\", linewidth=2)\n",
    "ax.plot(dr_cp, sa_cp, label=\"Standard CP\", color=\"#1f77b4\", linewidth=1.5,\n",
    "        marker=\"^\", markersize=3, alpha=0.7)\n",
    "ax.plot(dr_wcp, sa_wcp, label=\"WCP (DRE)\", color=\"#ff7f0e\", linewidth=1.5,\n",
    "        marker=\"s\", markersize=3, alpha=0.7)\n",
    "\n",
    "# Mark calibrated operating points on continuous curve\n",
    "for alpha, marker_label in [(0.1, \"10%\"), (0.2, \"20%\"), (0.3, \"30%\")]:\n",
    "    # Source-calibrated threshold\n",
    "    tau_src = weighted_quantile(cal_u, 1 - alpha)\n",
    "    defer_src = (test_u > tau_src).mean()\n",
    "    preds = np.argmax(test_nih_logits, axis=1)\n",
    "    m = compute_system_accuracy(preds, nih_test_labels, test_u > tau_src,\n",
    "                                 expert_accuracy=EXPERT_ACCURACY)\n",
    "    ax.plot(defer_src, m[\"system_accuracy\"], \"D\", color=\"#9467bd\", markersize=8)\n",
    "\n",
    "    # Weighted threshold\n",
    "    tau_w = weighted_quantile(cal_u, 1 - alpha, weights=cal_weights)\n",
    "    defer_w = (test_u > tau_w).mean()\n",
    "    m_w = compute_system_accuracy(preds, nih_test_labels, test_u > tau_w,\n",
    "                                   expert_accuracy=EXPERT_ACCURACY)\n",
    "    ax.plot(defer_w, m_w[\"system_accuracy\"], \"*\", color=\"#d62728\", markersize=12)\n",
    "\n",
    "# Dummy entries for legend\n",
    "ax.plot([], [], \"D\", color=\"#9467bd\", markersize=8, label=\"Source-cal (10/20/30%)\")\n",
    "ax.plot([], [], \"*\", color=\"#d62728\", markersize=12, label=\"DRE-weighted (10/20/30%)\")\n",
    "\n",
    "ax.set_xlabel(\"Deferral Rate\")\n",
    "ax.set_ylabel(\"System Accuracy\")\n",
    "ax.set_title(f\"Accuracy-Rejection Curve ({TARGET_PATHOLOGY})\")\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-0.02, 1.02)\n",
    "\n",
    "# Model accuracy on kept samples vs deferral\n",
    "ax = axes[1]\n",
    "ax.plot(dr_cont, ma_cont, label=\"Continuous (sweep)\", color=\"#2ca02c\", linewidth=2)\n",
    "ax.plot(dr_cp, ma_cp, label=\"Standard CP\", color=\"#1f77b4\", linewidth=1.5,\n",
    "        marker=\"^\", markersize=3, alpha=0.7)\n",
    "ax.plot(dr_wcp, ma_wcp, label=\"WCP (DRE)\", color=\"#ff7f0e\", linewidth=1.5,\n",
    "        marker=\"s\", markersize=3, alpha=0.7)\n",
    "ax.set_xlabel(\"Deferral Rate\")\n",
    "ax.set_ylabel(\"Model Accuracy (non-deferred)\")\n",
    "ax.set_title(f\"Selective Accuracy ({TARGET_PATHOLOGY})\")\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-0.02, 1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calibrated Operating Points\n",
    "\n",
    "Compare source-calibrated vs DRE-weighted thresholds.\n",
    "Under covariate shift, the weighted threshold should give actual\n",
    "deferral rates closer to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_alphas = np.array([0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.40, 0.50])\n",
    "model_preds = np.argmax(test_nih_logits, axis=1)\n",
    "\n",
    "rows = []\n",
    "for alpha in target_alphas:\n",
    "    # Source-calibrated\n",
    "    tau_src = weighted_quantile(cal_u, 1 - alpha)\n",
    "    defer_src = test_u > tau_src\n",
    "    m_src = compute_system_accuracy(\n",
    "        model_preds, nih_test_labels, defer_src, expert_accuracy=EXPERT_ACCURACY\n",
    "    )\n",
    "\n",
    "    # DRE-weighted\n",
    "    tau_w = weighted_quantile(cal_u, 1 - alpha, weights=cal_weights)\n",
    "    defer_w = test_u > tau_w\n",
    "    m_w = compute_system_accuracy(\n",
    "        model_preds, nih_test_labels, defer_w, expert_accuracy=EXPERT_ACCURACY\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        \"Target def\": f\"{alpha:.0%}\",\n",
    "        \"Src tau\": f\"{tau_src:.4f}\",\n",
    "        \"Src actual def\": f\"{m_src['deferral_rate']:.3f}\",\n",
    "        \"Src sys acc\": f\"{m_src['system_accuracy']:.4f}\",\n",
    "        \"Src model acc\": f\"{m_src['model_accuracy_on_kept']:.4f}\",\n",
    "        \"W tau\": f\"{tau_w:.4f}\",\n",
    "        \"W actual def\": f\"{m_w['deferral_rate']:.3f}\",\n",
    "        \"W sys acc\": f\"{m_w['system_accuracy']:.4f}\",\n",
    "        \"W model acc\": f\"{m_w['model_accuracy_on_kept']:.4f}\",\n",
    "    })\n",
    "\n",
    "df_op = pd.DataFrame(rows)\n",
    "print(f\"Calibrated operating points for {TARGET_PATHOLOGY}\")\n",
    "print(f\"Source-calibrated (Src) vs DRE-weighted (W)\")\n",
    "print(\"=\" * 120)\n",
    "print(df_op.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison with CP Methods at Matched Deferral Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sys_acc_at_deferral(dr_arr, sa_arr, target_dr):\n",
    "    \"\"\"Interpolate system accuracy at a target deferral rate.\"\"\"\n",
    "    order = np.argsort(dr_arr)\n",
    "    dr_sorted = dr_arr[order]\n",
    "    sa_sorted = sa_arr[order]\n",
    "    return float(np.interp(target_dr, dr_sorted, sa_sorted))\n",
    "\n",
    "\n",
    "target_deferrals = [0.05, 0.10, 0.20, 0.30]\n",
    "rows = []\n",
    "for td in target_deferrals:\n",
    "    rows.append({\n",
    "        \"Deferral\": f\"{td:.0%}\",\n",
    "        \"Continuous\": f\"{sys_acc_at_deferral(dr_cont, sa_cont, td):.4f}\",\n",
    "        \"Standard CP\": f\"{sys_acc_at_deferral(dr_cp, sa_cp, td):.4f}\",\n",
    "        \"WCP (DRE)\": f\"{sys_acc_at_deferral(dr_wcp, sa_wcp, td):.4f}\",\n",
    "    })\n",
    "\n",
    "print(f\"System accuracy at matched deferral rates ({TARGET_PATHOLOGY})\")\n",
    "print(\"=\" * 60)\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Uncertainty Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Cal vs test uncertainty distributions\n",
    "ax = axes[0]\n",
    "ax.hist(cal_u, bins=50, alpha=0.6, label=\"CheXpert cal\", density=True, color=\"#1f77b4\")\n",
    "ax.hist(test_u, bins=50, alpha=0.6, label=\"NIH test\", density=True, color=\"#ff7f0e\")\n",
    "ax.set_xlabel(\"Uncertainty (1 - max softmax)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Uncertainty Distributions\")\n",
    "ax.legend()\n",
    "\n",
    "# Weighted cal vs test\n",
    "ax = axes[1]\n",
    "ax.hist(cal_u, bins=50, weights=cal_weights / cal_weights.sum() * len(cal_u),\n",
    "        alpha=0.6, label=\"CheXpert cal (DRE-weighted)\", density=True, color=\"#2ca02c\")\n",
    "ax.hist(test_u, bins=50, alpha=0.6, label=\"NIH test\", density=True, color=\"#ff7f0e\")\n",
    "ax.set_xlabel(\"Uncertainty (1 - max softmax)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"DRE-Weighted Cal vs Test\")\n",
    "ax.legend()\n",
    "\n",
    "# Calibrated thresholds\n",
    "ax = axes[2]\n",
    "alphas_sweep = np.linspace(0.01, 0.5, 50)\n",
    "tau_src = [weighted_quantile(cal_u, 1 - a) for a in alphas_sweep]\n",
    "tau_w = [weighted_quantile(cal_u, 1 - a, weights=cal_weights) for a in alphas_sweep]\n",
    "actual_def_src = [(test_u > t).mean() for t in tau_src]\n",
    "actual_def_w = [(test_u > t).mean() for t in tau_w]\n",
    "\n",
    "ax.plot(alphas_sweep, actual_def_src, label=\"Source-calibrated\", color=\"#9467bd\", linewidth=1.5)\n",
    "ax.plot(alphas_sweep, actual_def_w, label=\"DRE-weighted\", color=\"#d62728\", linewidth=1.5)\n",
    "ax.plot(alphas_sweep, alphas_sweep, \"k--\", alpha=0.5, label=\"Ideal\")\n",
    "ax.set_xlabel(\"Target deferral rate (alpha)\")\n",
    "ax.set_ylabel(\"Actual deferral rate on NIH\")\n",
    "ax.set_title(\"Calibration: Target vs Actual\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Pathology Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "multi_results = []\nmulti_arcs = {}  # Cache ARC data for plotting in next cell\n\nfor pathology in COMMON_PATHOLOGIES:\n    print(f\"Processing {pathology}...\", end=\" \", flush=True)\n\n    # Binary labels\n    c_feats, c_labels, _ = extract_binary_labels(\n        chexpert.features, chexpert.labels, COMMON_PATHOLOGIES, pathology\n    )\n    n_feats, n_labels, _ = extract_binary_labels(\n        nih.features, nih.labels, COMMON_PATHOLOGIES, pathology\n    )\n\n    # Splits\n    c_tr_f, c_tmp_f, c_tr_l, c_tmp_l = train_test_split(\n        c_feats, c_labels, test_size=0.4, random_state=SEED, stratify=c_labels\n    )\n    c_cal_f, _, c_cal_l, _ = train_test_split(\n        c_tmp_f, c_tmp_l, test_size=0.5, random_state=SEED, stratify=c_tmp_l\n    )\n    _, n_te_f, _, n_te_l = train_test_split(\n        n_feats, n_labels, test_size=0.5, random_state=SEED, stratify=n_labels\n    )\n\n    # Classifier\n    sc = StandardScaler()\n    Xtr = sc.fit_transform(c_tr_f)\n    Xcal = sc.transform(c_cal_f)\n    Xte = sc.transform(n_te_f)\n\n    model = LogisticRegression(solver=\"lbfgs\", max_iter=1000, C=1.0, random_state=SEED)\n    model.fit(Xtr, c_tr_l)\n    nih_auc = roc_auc_score(n_te_l, model.predict_proba(Xte)[:, 1])\n\n    def _logits(m, X):\n        d = m.decision_function(X)\n        return np.column_stack([-d, d])\n\n    c_lg = _logits(model, Xcal)\n    t_lg = _logits(model, Xte)\n\n    # DRE\n    d = AdaptiveDRE(n_components=4, weight_clip=20.0, random_state=SEED)\n    d.fit(c_cal_f, nih_pool_feats_all)\n    cw = d.compute_weights(c_cal_f)\n    tw = d.compute_weights(n_te_f)\n\n    # Uncertainty scores\n    cu = compute_uncertainty(c_lg)\n    tu = compute_uncertainty(t_lg)\n    m_preds = np.argmax(t_lg, axis=1)\n\n    # Continuous ARC\n    dr_c, sa_c, ma_c = accuracy_rejection_curve(tu, t_lg, n_te_l)\n\n    # CP-based ARCs (optimized: scores computed once per method)\n    dr_std, sa_std, _ = cp_deferral_curve(c_lg, c_cal_l, t_lg, n_te_l, n_alphas=30)\n    dr_w, sa_w, _ = cp_deferral_curve(\n        c_lg, c_cal_l, t_lg, n_te_l, cal_weights=cw, test_weights=tw, n_alphas=30\n    )\n\n    # Cache ARC data for plotting\n    multi_arcs[pathology] = {\n        \"dr_c\": dr_c, \"sa_c\": sa_c,\n        \"dr_std\": dr_std, \"sa_std\": sa_std,\n        \"dr_w\": dr_w, \"sa_w\": sa_w,\n    }\n\n    row = {\n        \"Pathology\": pathology,\n        \"NIH AUC\": f\"{nih_auc:.3f}\",\n        \"Model acc\": f\"{(m_preds == n_te_l).mean():.3f}\",\n    }\n\n    # System accuracy at fixed deferral rates\n    for td in [0.10, 0.20, 0.30]:\n        row[f\"Cont@{td:.0%}\"] = f\"{sys_acc_at_deferral(dr_c, sa_c, td):.3f}\"\n        row[f\"StdCP@{td:.0%}\"] = f\"{sys_acc_at_deferral(dr_std, sa_std, td):.3f}\"\n        row[f\"WCP@{td:.0%}\"] = f\"{sys_acc_at_deferral(dr_w, sa_w, td):.3f}\"\n\n    # Weighted calibration quality: target 20% deferral\n    tau_w20 = weighted_quantile(cu, 0.8, weights=cw)\n    actual_w20 = (tu > tau_w20).mean()\n    row[\"WCD target=20%\"] = f\"{actual_w20:.3f}\"\n\n    multi_results.append(row)\n    print(\n        f\"AUC={nih_auc:.3f}  \"\n        f\"Cont@20%={row['Cont@20%']}  StdCP@20%={row['StdCP@20%']}  \"\n        f\"WCP@20%={row['WCP@20%']}  WCD_actual_def={actual_w20:.3f}\"\n    )\n\ndf_multi = pd.DataFrame(multi_results)\nprint(f\"\\n{'=' * 130}\")\nprint(\"System accuracy at matched deferral rates (all pathologies)\")\nprint(f\"{'=' * 130}\")\nprint(df_multi.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-pathology bar chart at 20% deferral\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "pathology_names = [r[\"Pathology\"] for r in multi_results]\n",
    "x = np.arange(len(pathology_names))\n",
    "width = 0.25\n",
    "\n",
    "for i, (key, label) in enumerate([\n",
    "    (\"Cont@20%\", \"Continuous\"),\n",
    "    (\"StdCP@20%\", \"Standard CP\"),\n",
    "    (\"WCP@20%\", \"WCP (DRE)\"),\n",
    "]):\n",
    "    vals = [float(r[key]) for r in multi_results]\n",
    "    ax.bar(x + i * width, vals, width, label=label)\n",
    "\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(pathology_names, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"System Accuracy\")\n",
    "ax.set_title(\"System Accuracy at 20% Deferral Rate\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "ax.set_ylim(0.8, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Full ARC comparison for all pathologies (using cached data from cell above)\nfig, axes = plt.subplots(2, 4, figsize=(18, 8))\naxes = axes.flatten()\n\nfor idx, pathology in enumerate(COMMON_PATHOLOGIES):\n    arcs = multi_arcs[pathology]\n    ax = axes[idx]\n    ax.plot(arcs[\"dr_c\"], arcs[\"sa_c\"], color=\"#2ca02c\", linewidth=2, label=\"Continuous\")\n    ax.plot(arcs[\"dr_std\"], arcs[\"sa_std\"], color=\"#1f77b4\", linewidth=1.5, alpha=0.7, label=\"Std CP\")\n    ax.plot(arcs[\"dr_w\"], arcs[\"sa_w\"], color=\"#ff7f0e\", linewidth=1.5, alpha=0.7, label=\"WCP\")\n    ax.set_title(pathology, fontsize=10)\n    ax.set_xlim(-0.02, 1.02)\n    ax.grid(True, alpha=0.3)\n    if idx == 0:\n        ax.legend(fontsize=7)\n    if idx >= 4:\n        ax.set_xlabel(\"Deferral Rate\")\n    if idx % 4 == 0:\n        ax.set_ylabel(\"System Accuracy\")\n\n# Hide unused subplot\naxes[7].set_visible(False)\n\nfig.suptitle(\"Accuracy-Rejection Curves by Pathology\", fontsize=12, y=1.01)\nplt.tight_layout()\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wcp-l2d",
   "language": "python",
   "name": "wcp-l2d"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}