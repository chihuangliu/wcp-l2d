{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mondrian CP for WCP-L2D\n",
    "\n",
    "Class-conditional (Mondrian) conformal prediction to address the\n",
    "prevalence shift problem in binary CP deferral.\n",
    "\n",
    "**Key idea**: Instead of one quantile threshold for all classes,\n",
    "compute separate thresholds for class 0 (negative) and class 1 (positive).\n",
    "This naturally handles the massive prevalence shift between CheXpert and NIH,\n",
    "where standard CP produces overly conservative prediction sets (~95% deferral)\n",
    "and WCP has inconsistent behavior across pathologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torchcp.classification.score import RAPS\n",
    "\n",
    "from wcp_l2d.features import ExtractedFeatures\n",
    "from wcp_l2d.pathologies import COMMON_PATHOLOGIES\n",
    "from wcp_l2d.label_utils import extract_binary_labels\n",
    "from wcp_l2d.dre import AdaptiveDRE\n",
    "from wcp_l2d.conformal import ConformalPredictor, WeightedConformalPredictor\n",
    "from wcp_l2d.evaluation import (\n",
    "    compute_coverage,\n",
    "    compute_system_accuracy,\n",
    "    _predictions_from_sets,\n",
    "    DeferralResult,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "EXPERT_ACCURACY = 0.85\n",
    "FEATURE_DIR = Path(\"../data/features\")\n",
    "TARGET_PATHOLOGY = \"Effusion\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Load features, extract binary labels, split data, train classifier, fit DRE.\n",
    "Same pipeline as wcp_experiment.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "chexpert = ExtractedFeatures.load(\n",
    "    FEATURE_DIR / \"chexpert_densenet121-res224-chex_features.npz\"\n",
    ")\n",
    "nih = ExtractedFeatures.load(FEATURE_DIR / \"nih_densenet121-res224-chex_features.npz\")\n",
    "\n",
    "# Binary labels for target pathology\n",
    "chex_feats, chex_labels, _ = extract_binary_labels(\n",
    "    chexpert.features, chexpert.labels, COMMON_PATHOLOGIES, TARGET_PATHOLOGY\n",
    ")\n",
    "nih_feats, nih_labels, _ = extract_binary_labels(\n",
    "    nih.features, nih.labels, COMMON_PATHOLOGIES, TARGET_PATHOLOGY\n",
    ")\n",
    "\n",
    "# Splits\n",
    "chex_train_feats, chex_temp_feats, chex_train_labels, chex_temp_labels = (\n",
    "    train_test_split(\n",
    "        chex_feats, chex_labels, test_size=0.4, random_state=SEED, stratify=chex_labels\n",
    "    )\n",
    ")\n",
    "chex_cal_feats, chex_test_feats, chex_cal_labels, chex_test_labels = train_test_split(\n",
    "    chex_temp_feats,\n",
    "    chex_temp_labels,\n",
    "    test_size=0.5,\n",
    "    random_state=SEED,\n",
    "    stratify=chex_temp_labels,\n",
    ")\n",
    "\n",
    "# NIH pool (unlabeled, for DRE) + test\n",
    "rng = np.random.RandomState(SEED)\n",
    "nih_all_perm = rng.permutation(len(nih.features))\n",
    "nih_pool_feats_all = nih.features[nih_all_perm[: len(nih.features) // 2]]\n",
    "\n",
    "_, nih_test_feats, _, nih_test_labels = train_test_split(\n",
    "    nih_feats, nih_labels, test_size=0.5, random_state=SEED, stratify=nih_labels\n",
    ")\n",
    "\n",
    "print(f\"CheXpert cal: {len(chex_cal_labels)} (prev={chex_cal_labels.mean():.3f})\")\n",
    "print(f\"NIH test:     {len(nih_test_labels)} (prev={nih_test_labels.mean():.3f})\")\n",
    "print(f\"Prevalence ratio: {chex_cal_labels.mean() / nih_test_labels.mean():.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train binary classifier\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(chex_train_feats)\n",
    "X_cal = scaler.transform(chex_cal_feats)\n",
    "X_test_chex = scaler.transform(chex_test_feats)\n",
    "X_test_nih = scaler.transform(nih_test_feats)\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    solver=\"lbfgs\", max_iter=1000, C=1.0, random_state=SEED\n",
    ")\n",
    "clf.fit(X_train, chex_train_labels)\n",
    "\n",
    "\n",
    "def get_binary_logits(clf, X):\n",
    "    d = clf.decision_function(X)\n",
    "    return np.column_stack([-d, d])\n",
    "\n",
    "\n",
    "cal_logits = get_binary_logits(clf, X_cal)\n",
    "test_chex_logits = get_binary_logits(clf, X_test_chex)\n",
    "test_nih_logits = get_binary_logits(clf, X_test_nih)\n",
    "\n",
    "# DRE\n",
    "dre = AdaptiveDRE(n_components=4, weight_clip=20.0, random_state=SEED)\n",
    "dre.fit(chex_cal_feats, nih_pool_feats_all)\n",
    "cal_weights = dre.compute_weights(chex_cal_feats)\n",
    "test_nih_weights = dre.compute_weights(nih_test_feats)\n",
    "\n",
    "diag = dre.diagnostics(chex_cal_feats)\n",
    "print(f\"Classifier NIH AUC: {roc_auc_score(nih_test_labels, clf.predict_proba(X_test_nih)[:, 1]):.4f}\")\n",
    "print(f\"DRE ESS: {diag.ess:.0f}/{len(cal_weights)} = {diag.ess_fraction:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mondrian Conformal Prediction\n",
    "\n",
    "Standard CP uses a single quantile threshold for all classes:\n",
    "$$C(x) = \\{y : s(x,y) \\leq \\hat{q}\\}$$\n",
    "\n",
    "Mondrian CP computes **class-conditional thresholds**:\n",
    "$$C(x) = \\{y : s(x,y) \\leq \\hat{q}_y\\}$$\n",
    "\n",
    "where $\\hat{q}_y$ is calibrated only on samples with true label $y$.\n",
    "This gives **class-conditional coverage**: $P(Y \\in C(X) \\mid Y=y) \\geq 1-\\alpha$ for each $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MondrianConformalPredictor:\n",
    "    \"\"\"Class-conditional (Mondrian) conformal prediction using RAPS.\n",
    "\n",
    "    Calibrates separate quantile thresholds per class, providing\n",
    "    class-conditional coverage guarantees.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, penalty=0.1, kreg=1, randomized=False):\n",
    "        self.score_fn = RAPS(penalty=penalty, kreg=kreg, randomized=randomized)\n",
    "        self.q_hats = {}  # {class_id: q_hat}\n",
    "\n",
    "    def calibrate(self, logits, labels, alpha=0.1):\n",
    "        \"\"\"Calibrate with class-conditional quantiles.\"\"\"\n",
    "        logits_t = torch.tensor(logits, dtype=torch.float32)\n",
    "        labels_t = torch.tensor(labels, dtype=torch.long)\n",
    "        scores = self.score_fn(logits_t, labels_t).numpy()\n",
    "\n",
    "        for c in np.unique(labels):\n",
    "            mask = labels == c\n",
    "            class_scores = np.sort(scores[mask])\n",
    "            n = len(class_scores)\n",
    "            k = math.ceil((n + 1) * (1 - alpha))\n",
    "            self.q_hats[int(c)] = (\n",
    "                float(class_scores[k - 1]) if k <= n else float(\"inf\")\n",
    "            )\n",
    "\n",
    "        return self.q_hats\n",
    "\n",
    "    def predict(self, logits):\n",
    "        \"\"\"Generate prediction sets with per-class thresholds.\"\"\"\n",
    "        logits_t = torch.tensor(logits, dtype=torch.float32)\n",
    "        all_scores = self.score_fn(logits_t).numpy()  # [N, K]\n",
    "\n",
    "        N, K = all_scores.shape\n",
    "        prediction_sets = np.zeros((N, K), dtype=np.int32)\n",
    "        for c in range(K):\n",
    "            if c in self.q_hats:\n",
    "                prediction_sets[:, c] = (all_scores[:, c] <= self.q_hats[c]).astype(\n",
    "                    np.int32\n",
    "                )\n",
    "        return prediction_sets\n",
    "\n",
    "\n",
    "class MondrianWeightedConformalPredictor:\n",
    "    \"\"\"Class-conditional weighted conformal prediction.\n",
    "\n",
    "    Combines Mondrian (per-class) calibration with importance-weighted\n",
    "    quantiles from DRE, addressing both covariate and prevalence shift.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, penalty=0.1, kreg=1, randomized=False):\n",
    "        self.score_fn = RAPS(penalty=penalty, kreg=kreg, randomized=randomized)\n",
    "        self.cal_data = {}  # {class_id: {scores, weights}}\n",
    "\n",
    "    def calibrate(self, logits, labels, weights):\n",
    "        \"\"\"Calibrate with per-class weighted scores.\"\"\"\n",
    "        logits_t = torch.tensor(logits, dtype=torch.float32)\n",
    "        labels_t = torch.tensor(labels, dtype=torch.long)\n",
    "        scores = self.score_fn(logits_t, labels_t).numpy()\n",
    "\n",
    "        for c in np.unique(labels):\n",
    "            mask = labels == c\n",
    "            class_scores = scores[mask]\n",
    "            class_weights = weights[mask]\n",
    "            sort_idx = np.argsort(class_scores)\n",
    "            self.cal_data[int(c)] = {\n",
    "                \"scores\": class_scores[sort_idx],\n",
    "                \"weights\": class_weights[sort_idx],\n",
    "            }\n",
    "\n",
    "    def predict(self, logits, test_weights, alpha=0.1):\n",
    "        \"\"\"Generate prediction sets with per-class weighted quantiles.\"\"\"\n",
    "        logits_t = torch.tensor(logits, dtype=torch.float32)\n",
    "        all_scores = self.score_fn(logits_t).numpy()  # [N, K]\n",
    "\n",
    "        N, K = all_scores.shape\n",
    "        prediction_sets = np.zeros((N, K), dtype=np.int32)\n",
    "\n",
    "        for c in range(K):\n",
    "            if c not in self.cal_data:\n",
    "                continue\n",
    "\n",
    "            cal_scores = self.cal_data[c][\"scores\"]\n",
    "            cal_weights = self.cal_data[c][\"weights\"]\n",
    "            n_cal = len(cal_scores)\n",
    "\n",
    "            # Weighted quantile: [N_test, n_cal + 1] (last = test point at inf)\n",
    "            cal_w = cal_weights[np.newaxis, :]  # [1, n_cal]\n",
    "            test_w = test_weights[:, np.newaxis]  # [N, 1]\n",
    "\n",
    "            all_w = np.concatenate(\n",
    "                [np.broadcast_to(cal_w, (N, n_cal)), test_w], axis=1\n",
    "            )\n",
    "            p = all_w / all_w.sum(axis=1, keepdims=True)\n",
    "            cumprob = np.cumsum(p[:, :n_cal], axis=1)\n",
    "\n",
    "            target = 1 - alpha\n",
    "            reached = cumprob >= target\n",
    "            has_any = reached.any(axis=1)\n",
    "            first_idx = np.argmax(reached, axis=1)\n",
    "\n",
    "            q_hat = np.where(has_any, cal_scores[first_idx], np.inf)\n",
    "            prediction_sets[:, c] = (all_scores[:, c] <= q_hat).astype(np.int32)\n",
    "\n",
    "        return prediction_sets\n",
    "\n",
    "\n",
    "print(\"Mondrian predictors defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate All Methods on Target Pathology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cp_method(method_name, predictor_factory, alphas, expert_accuracy=EXPERT_ACCURACY):\n",
    "    \"\"\"Generic evaluation loop for any CP method.\"\"\"\n",
    "    results = []\n",
    "    for alpha in alphas:\n",
    "        pred_sets = predictor_factory(alpha)\n",
    "        cov = compute_coverage(pred_sets, nih_test_labels)\n",
    "        preds, defer_mask = _predictions_from_sets(pred_sets, test_nih_logits)\n",
    "        sys = compute_system_accuracy(\n",
    "            preds, nih_test_labels, defer_mask, expert_accuracy=expert_accuracy\n",
    "        )\n",
    "        results.append(\n",
    "            DeferralResult(\n",
    "                method=method_name,\n",
    "                alpha_or_threshold=float(alpha),\n",
    "                system_accuracy=sys[\"system_accuracy\"],\n",
    "                deferral_rate=sys[\"deferral_rate\"],\n",
    "                coverage_rate=cov[\"coverage_rate\"],\n",
    "                average_set_size=cov[\"average_set_size\"],\n",
    "                model_accuracy_on_kept=sys[\"model_accuracy_on_kept\"],\n",
    "                n_total=len(nih_test_labels),\n",
    "                n_deferred=sys[\"n_deferred\"],\n",
    "            )\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "alphas = np.linspace(0.01, 0.5, 50)\n",
    "\n",
    "\n",
    "# Standard CP\n",
    "def std_cp_factory(alpha):\n",
    "    cp = ConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n",
    "    cp.calibrate(cal_logits, chex_cal_labels, alpha=alpha)\n",
    "    return cp.predict(test_nih_logits)\n",
    "\n",
    "\n",
    "# WCP\n",
    "def wcp_factory(alpha):\n",
    "    wcp = WeightedConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n",
    "    wcp.calibrate(cal_logits, chex_cal_labels, cal_weights)\n",
    "    return wcp.predict(test_nih_logits, test_nih_weights, alpha=alpha)\n",
    "\n",
    "\n",
    "# Mondrian CP\n",
    "def mondrian_cp_factory(alpha):\n",
    "    mcp = MondrianConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n",
    "    mcp.calibrate(cal_logits, chex_cal_labels, alpha=alpha)\n",
    "    return mcp.predict(test_nih_logits)\n",
    "\n",
    "\n",
    "# Mondrian WCP\n",
    "def mondrian_wcp_factory(alpha):\n",
    "    mwcp = MondrianWeightedConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n",
    "    mwcp.calibrate(cal_logits, chex_cal_labels, cal_weights)\n",
    "    return mwcp.predict(test_nih_logits, test_nih_weights, alpha=alpha)\n",
    "\n",
    "\n",
    "std_results = evaluate_cp_method(\"Standard CP\", std_cp_factory, alphas)\n",
    "wcp_results = evaluate_cp_method(\"WCP\", wcp_factory, alphas)\n",
    "mcp_results = evaluate_cp_method(\"Mondrian CP\", mondrian_cp_factory, alphas)\n",
    "mwcp_results = evaluate_cp_method(\"Mondrian WCP\", mondrian_wcp_factory, alphas)\n",
    "\n",
    "print(\"All methods evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary at alpha=0.1\n",
    "alpha_target = 0.1\n",
    "rows = []\n",
    "for name, res_list in [\n",
    "    (\"Standard CP\", std_results),\n",
    "    (\"WCP\", wcp_results),\n",
    "    (\"Mondrian CP\", mcp_results),\n",
    "    (\"Mondrian WCP\", mwcp_results),\n",
    "]:\n",
    "    r = min(res_list, key=lambda r: abs(r.alpha_or_threshold - alpha_target))\n",
    "    rows.append(\n",
    "        {\n",
    "            \"Method\": name,\n",
    "            \"Coverage\": f\"{r.coverage_rate:.4f}\",\n",
    "            \"Avg Set Size\": f\"{r.average_set_size:.3f}\",\n",
    "            \"Deferral Rate\": f\"{r.deferral_rate:.4f}\",\n",
    "            \"System Acc\": f\"{r.system_accuracy:.4f}\",\n",
    "            \"Model Acc (kept)\": f\"{r.model_accuracy_on_kept:.4f}\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"\\n{TARGET_PATHOLOGY} — Results at alpha={alpha_target} (target coverage >= {1-alpha_target:.0%})\")\n",
    "print(\"=\" * 90)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Class-Conditional Coverage Analysis\n",
    "\n",
    "Examine coverage separately for positive and negative samples to understand\n",
    "how each method handles the prevalence shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_conditional_analysis(pred_sets, logits, labels, method_name, alpha=0.1):\n",
    "    \"\"\"Coverage and deferral broken down by true class.\"\"\"\n",
    "    set_sizes = pred_sets.sum(axis=1)\n",
    "    covered = pred_sets[np.arange(len(labels)), labels].astype(bool)\n",
    "    deferred = set_sizes != 1\n",
    "\n",
    "    rows = []\n",
    "    for c, c_name in [(0, \"Negative\"), (1, \"Positive\")]:\n",
    "        mask = labels == c\n",
    "        n = mask.sum()\n",
    "        rows.append(\n",
    "            {\n",
    "                \"Method\": method_name,\n",
    "                \"Class\": c_name,\n",
    "                \"N\": n,\n",
    "                \"Coverage\": f\"{covered[mask].mean():.4f}\",\n",
    "                \"Deferral\": f\"{deferred[mask].mean():.4f}\",\n",
    "                \"Avg |C|\": f\"{set_sizes[mask].mean():.3f}\",\n",
    "            }\n",
    "        )\n",
    "    rows.append(\n",
    "        {\n",
    "            \"Method\": method_name,\n",
    "            \"Class\": \"Overall\",\n",
    "            \"N\": len(labels),\n",
    "            \"Coverage\": f\"{covered.mean():.4f}\",\n",
    "            \"Deferral\": f\"{deferred.mean():.4f}\",\n",
    "            \"Avg |C|\": f\"{set_sizes.mean():.3f}\",\n",
    "        }\n",
    "    )\n",
    "    return rows\n",
    "\n",
    "\n",
    "alpha_target = 0.1\n",
    "all_rows = []\n",
    "\n",
    "for name, PredClass, use_weights in [\n",
    "    (\"Standard CP\", ConformalPredictor, False),\n",
    "    (\"WCP\", WeightedConformalPredictor, True),\n",
    "    (\"Mondrian CP\", MondrianConformalPredictor, False),\n",
    "    (\"Mondrian WCP\", MondrianWeightedConformalPredictor, True),\n",
    "]:\n",
    "    pred = PredClass(penalty=0.1, kreg=1, randomized=False)\n",
    "    if use_weights:\n",
    "        pred.calibrate(cal_logits, chex_cal_labels, cal_weights)\n",
    "        ps = pred.predict(test_nih_logits, test_nih_weights, alpha=alpha_target)\n",
    "    else:\n",
    "        pred.calibrate(cal_logits, chex_cal_labels, alpha=alpha_target)\n",
    "        ps = pred.predict(test_nih_logits)\n",
    "    all_rows.extend(class_conditional_analysis(ps, test_nih_logits, nih_test_labels, name))\n",
    "\n",
    "df_cc = pd.DataFrame(all_rows)\n",
    "print(f\"\\nClass-conditional analysis for {TARGET_PATHOLOGY} at alpha={alpha_target}\")\n",
    "print(\"=\" * 80)\n",
    "print(df_cc.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mondrian Quantile Inspection\n",
    "\n",
    "Compare the quantile thresholds computed by each method to understand\n",
    "why prediction sets differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_target = 0.1\n",
    "\n",
    "# Standard CP\n",
    "std_cp = ConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n",
    "std_q = std_cp.calibrate(cal_logits, chex_cal_labels, alpha=alpha_target)\n",
    "\n",
    "# Mondrian CP\n",
    "mon_cp = MondrianConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n",
    "mon_qs = mon_cp.calibrate(cal_logits, chex_cal_labels, alpha=alpha_target)\n",
    "\n",
    "print(f\"Quantile thresholds at alpha={alpha_target}:\")\n",
    "print(f\"  Standard CP:  q_hat = {std_q:.6f} (single threshold for all classes)\")\n",
    "print(f\"  Mondrian CP:  q_hat_0 (neg) = {mon_qs[0]:.6f}\")\n",
    "print(f\"                q_hat_1 (pos) = {mon_qs[1]:.6f}\")\n",
    "print()\n",
    "\n",
    "# Score distributions by class\n",
    "score_fn = RAPS(penalty=0.1, kreg=1, randomized=False)\n",
    "cal_scores = score_fn(\n",
    "    torch.tensor(cal_logits, dtype=torch.float32),\n",
    "    torch.tensor(chex_cal_labels, dtype=torch.long),\n",
    ").numpy()\n",
    "\n",
    "print(\"Calibration score statistics:\")\n",
    "for c, name in [(0, \"Negative\"), (1, \"Positive\")]:\n",
    "    s = cal_scores[chex_cal_labels == c]\n",
    "    print(\n",
    "        f\"  Class {c} ({name}): n={len(s)}, \"\n",
    "        f\"mean={s.mean():.4f}, std={s.std():.4f}, \"\n",
    "        f\"median={np.median(s):.4f}, \"\n",
    "        f\"p10={np.percentile(s, 10):.4f}, p90={np.percentile(s, 90):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize calibration score distributions by class\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for ax, (c, name) in zip(axes, [(0, \"Negative\"), (1, \"Positive\")]):\n",
    "    s = cal_scores[chex_cal_labels == c]\n",
    "    ax.hist(s, bins=50, edgecolor=\"black\", linewidth=0.3, alpha=0.7)\n",
    "    ax.axvline(std_q, color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Std q={std_q:.3f}\")\n",
    "    ax.axvline(\n",
    "        mon_qs[c], color=\"green\", linestyle=\"-.\", linewidth=1.5, label=f\"Mon q_{c}={mon_qs[c]:.3f}\"\n",
    "    )\n",
    "    ax.set_xlabel(\"RAPS Score\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"Cal Scores — {name} (n={len(s)})\")\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Coverage vs Alpha Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "methods = {\n",
    "    \"Standard CP\": std_results,\n",
    "    \"WCP\": wcp_results,\n",
    "    \"Mondrian CP\": mcp_results,\n",
    "    \"Mondrian WCP\": mwcp_results,\n",
    "}\n",
    "colors = {\n",
    "    \"Standard CP\": \"#1f77b4\",\n",
    "    \"WCP\": \"#ff7f0e\",\n",
    "    \"Mondrian CP\": \"#2ca02c\",\n",
    "    \"Mondrian WCP\": \"#d62728\",\n",
    "}\n",
    "\n",
    "# Coverage vs alpha\n",
    "ax = axes[0]\n",
    "for name, res in methods.items():\n",
    "    a = [r.alpha_or_threshold for r in res]\n",
    "    c = [r.coverage_rate for r in res]\n",
    "    ax.plot(a, c, label=name, color=colors[name], linewidth=1.5, marker=\"o\", markersize=2)\n",
    "\n",
    "ax.plot(alphas, 1 - alphas, \"k--\", alpha=0.5, linewidth=1.5, label=r\"Ideal $1-\\alpha$\")\n",
    "ax.set_xlabel(r\"$\\alpha$\")\n",
    "ax.set_ylabel(\"Coverage\")\n",
    "ax.set_title(f\"Coverage vs Alpha ({TARGET_PATHOLOGY}, NIH test)\")\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Deferral vs alpha\n",
    "ax = axes[1]\n",
    "for name, res in methods.items():\n",
    "    a = [r.alpha_or_threshold for r in res]\n",
    "    d = [r.deferral_rate for r in res]\n",
    "    ax.plot(a, d, label=name, color=colors[name], linewidth=1.5, marker=\"o\", markersize=2)\n",
    "\n",
    "ax.set_xlabel(r\"$\\alpha$\")\n",
    "ax.set_ylabel(\"Deferral Rate\")\n",
    "ax.set_title(f\"Deferral Rate vs Alpha ({TARGET_PATHOLOGY}, NIH test)\")\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Pathology Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_target = 0.1\n",
    "all_results = []\n",
    "\n",
    "for pathology in COMMON_PATHOLOGIES:\n",
    "    # Binary labels\n",
    "    c_feats, c_labels, _ = extract_binary_labels(\n",
    "        chexpert.features, chexpert.labels, COMMON_PATHOLOGIES, pathology\n",
    "    )\n",
    "    n_feats, n_labels, _ = extract_binary_labels(\n",
    "        nih.features, nih.labels, COMMON_PATHOLOGIES, pathology\n",
    "    )\n",
    "\n",
    "    # Splits\n",
    "    c_tr_f, c_tmp_f, c_tr_l, c_tmp_l = train_test_split(\n",
    "        c_feats, c_labels, test_size=0.4, random_state=SEED, stratify=c_labels\n",
    "    )\n",
    "    c_cal_f, _, c_cal_l, _ = train_test_split(\n",
    "        c_tmp_f, c_tmp_l, test_size=0.5, random_state=SEED, stratify=c_tmp_l\n",
    "    )\n",
    "    _, n_te_f, _, n_te_l = train_test_split(\n",
    "        n_feats, n_labels, test_size=0.5, random_state=SEED, stratify=n_labels\n",
    "    )\n",
    "\n",
    "    # Classifier\n",
    "    sc = StandardScaler()\n",
    "    Xtr = sc.fit_transform(c_tr_f)\n",
    "    Xcal = sc.transform(c_cal_f)\n",
    "    Xte = sc.transform(n_te_f)\n",
    "\n",
    "    model = LogisticRegression(solver=\"lbfgs\", max_iter=1000, C=1.0, random_state=SEED)\n",
    "    model.fit(Xtr, c_tr_l)\n",
    "    nih_auc = roc_auc_score(n_te_l, model.predict_proba(Xte)[:, 1])\n",
    "\n",
    "    def _logits(m, X):\n",
    "        d = m.decision_function(X)\n",
    "        return np.column_stack([-d, d])\n",
    "\n",
    "    c_lg = _logits(model, Xcal)\n",
    "    t_lg = _logits(model, Xte)\n",
    "\n",
    "    # DRE\n",
    "    d = AdaptiveDRE(n_components=4, weight_clip=20.0, random_state=SEED)\n",
    "    d.fit(c_cal_f, nih_pool_feats_all)\n",
    "    cw = d.compute_weights(c_cal_f)\n",
    "    tw = d.compute_weights(n_te_f)\n",
    "\n",
    "    row = {\n",
    "        \"Pathology\": pathology,\n",
    "        \"NIH AUC\": f\"{nih_auc:.3f}\",\n",
    "        \"NIH prev\": f\"{n_te_l.mean():.3f}\",\n",
    "        \"Cal prev\": f\"{c_cal_l.mean():.3f}\",\n",
    "    }\n",
    "\n",
    "    # Evaluate all 4 methods\n",
    "    for method_name, PredClass, use_weights in [\n",
    "        (\"Std\", ConformalPredictor, False),\n",
    "        (\"WCP\", WeightedConformalPredictor, True),\n",
    "        (\"Mon\", MondrianConformalPredictor, False),\n",
    "        (\"MonW\", MondrianWeightedConformalPredictor, True),\n",
    "    ]:\n",
    "        pred = PredClass(penalty=0.1, kreg=1, randomized=False)\n",
    "        if use_weights:\n",
    "            pred.calibrate(c_lg, c_cal_l, cw)\n",
    "            ps = pred.predict(t_lg, tw, alpha=alpha_target)\n",
    "        else:\n",
    "            pred.calibrate(c_lg, c_cal_l, alpha=alpha_target)\n",
    "            ps = pred.predict(t_lg)\n",
    "\n",
    "        cov = ps[np.arange(len(n_te_l)), n_te_l].mean()\n",
    "        defer = (ps.sum(axis=1) != 1).mean()\n",
    "        row[f\"{method_name} Cov\"] = f\"{cov:.3f}\"\n",
    "        row[f\"{method_name} Def\"] = f\"{defer:.3f}\"\n",
    "\n",
    "    all_results.append(row)\n",
    "    print(\n",
    "        f\"{pathology:<16} AUC={nih_auc:.3f}  \"\n",
    "        f\"Std={row['Std Cov']}/{row['Std Def']}  \"\n",
    "        f\"WCP={row['WCP Cov']}/{row['WCP Def']}  \"\n",
    "        f\"Mon={row['Mon Cov']}/{row['Mon Def']}  \"\n",
    "        f\"MonW={row['MonW Cov']}/{row['MonW Def']}\"\n",
    "    )\n",
    "\n",
    "df_all = pd.DataFrame(all_results)\n",
    "print(f\"\\n{'=' * 120}\")\n",
    "print(f\"Multi-pathology comparison at alpha={alpha_target} (Coverage/Deferral)\")\n",
    "print(f\"{'=' * 120}\")\n",
    "print(df_all.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: deferral rates across pathologies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "pathology_names = [r[\"Pathology\"] for r in all_results]\n",
    "x = np.arange(len(pathology_names))\n",
    "width = 0.2\n",
    "\n",
    "# Deferral rates\n",
    "ax = axes[0]\n",
    "for i, (key, label) in enumerate(\n",
    "    [(\"Std Def\", \"Standard CP\"), (\"WCP Def\", \"WCP\"), (\"Mon Def\", \"Mondrian CP\"), (\"MonW Def\", \"Mondrian WCP\")]\n",
    "):\n",
    "    vals = [float(r[key]) for r in all_results]\n",
    "    ax.bar(x + i * width, vals, width, label=label)\n",
    "\n",
    "ax.set_xticks(x + 1.5 * width)\n",
    "ax.set_xticklabels(pathology_names, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Deferral Rate\")\n",
    "ax.set_title(f\"Deferral Rate by Pathology (alpha={alpha_target})\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Coverage\n",
    "ax = axes[1]\n",
    "for i, (key, label) in enumerate(\n",
    "    [(\"Std Cov\", \"Standard CP\"), (\"WCP Cov\", \"WCP\"), (\"Mon Cov\", \"Mondrian CP\"), (\"MonW Cov\", \"Mondrian WCP\")]\n",
    "):\n",
    "    vals = [float(r[key]) for r in all_results]\n",
    "    ax.bar(x + i * width, vals, width, label=label)\n",
    "\n",
    "ax.axhline(y=0.9, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Target (90%)\")\n",
    "ax.set_xticks(x + 1.5 * width)\n",
    "ax.set_xticklabels(pathology_names, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Coverage\")\n",
    "ax.set_title(f\"Coverage by Pathology (alpha={alpha_target})\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "ax.set_ylim(0.7, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wcp-l2d",
   "language": "python",
   "name": "wcp-l2d"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
