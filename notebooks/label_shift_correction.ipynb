{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Label-Shift Correction for WCP-L2D\n\nThe CheXpert-to-NIH shift includes massive **prevalence shift**\n(e.g., Effusion: 47% &rarr; 4%). The DRE-based WCP only corrects\n**covariate shift** (feature distribution), not label shift.\n\nThis notebook implements two label-shift correction approaches:\n\n1. **Prior-adjusted CP** &mdash; adjust classifier logits for prevalence shift\n   via Bayes' rule, then run standard CP on corrected posteriors.\n2. **Label-shift WCP** &mdash; use per-class importance weights\n   $w_i = p_{\\text{target}}(y_i) / p_{\\text{source}}(y_i)$ in the weighted quantile,\n   with per-class test weights.\n\nBoth require an estimate of the target prevalence. We compare two estimators:\n- **BBSE** (Black-Box Shift Estimation) &mdash; confusion matrix inversion (can fail with extreme shifts)\n- **MLLS** (Maximum Likelihood Label Shift) &mdash; EM algorithm, more robust\n\nWe also combine label-shift correction with DRE (covariate shift) for\na joint correction: **Prior-adjusted + DRE WCP**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torchcp.classification.score import RAPS\n",
    "\n",
    "from wcp_l2d.features import ExtractedFeatures\n",
    "from wcp_l2d.pathologies import COMMON_PATHOLOGIES\n",
    "from wcp_l2d.label_utils import extract_binary_labels\n",
    "from wcp_l2d.dre import AdaptiveDRE\n",
    "from wcp_l2d.conformal import ConformalPredictor, WeightedConformalPredictor\n",
    "from wcp_l2d.evaluation import (\n",
    "    compute_coverage,\n",
    "    compute_system_accuracy,\n",
    "    _predictions_from_sets,\n",
    "    DeferralResult,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "EXPERT_ACCURACY = 0.85\n",
    "FEATURE_DIR = Path(\"../data/features\")\n",
    "TARGET_PATHOLOGY = \"Effusion\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "chexpert = ExtractedFeatures.load(\n",
    "    FEATURE_DIR / \"chexpert_densenet121-res224-chex_features.npz\"\n",
    ")\n",
    "nih = ExtractedFeatures.load(FEATURE_DIR / \"nih_densenet121-res224-chex_features.npz\")\n",
    "\n",
    "# Binary labels for target pathology\n",
    "chex_feats, chex_labels, _ = extract_binary_labels(\n",
    "    chexpert.features, chexpert.labels, COMMON_PATHOLOGIES, TARGET_PATHOLOGY\n",
    ")\n",
    "nih_feats, nih_labels, _ = extract_binary_labels(\n",
    "    nih.features, nih.labels, COMMON_PATHOLOGIES, TARGET_PATHOLOGY\n",
    ")\n",
    "\n",
    "# Splits\n",
    "chex_train_feats, chex_temp_feats, chex_train_labels, chex_temp_labels = (\n",
    "    train_test_split(\n",
    "        chex_feats, chex_labels, test_size=0.4, random_state=SEED, stratify=chex_labels\n",
    "    )\n",
    ")\n",
    "chex_cal_feats, chex_test_feats, chex_cal_labels, chex_test_labels = train_test_split(\n",
    "    chex_temp_feats,\n",
    "    chex_temp_labels,\n",
    "    test_size=0.5,\n",
    "    random_state=SEED,\n",
    "    stratify=chex_temp_labels,\n",
    ")\n",
    "\n",
    "# NIH pool (unlabeled, for DRE + BBSE) and test\n",
    "rng = np.random.RandomState(SEED)\n",
    "nih_all_perm = rng.permutation(len(nih.features))\n",
    "nih_pool_feats_all = nih.features[nih_all_perm[: len(nih.features) // 2]]\n",
    "\n",
    "_, nih_test_feats, _, nih_test_labels = train_test_split(\n",
    "    nih_feats, nih_labels, test_size=0.5, random_state=SEED, stratify=nih_labels\n",
    ")\n",
    "\n",
    "# Classifier + scaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(chex_train_feats)\n",
    "X_cal = scaler.transform(chex_cal_feats)\n",
    "X_test_nih = scaler.transform(nih_test_feats)\n",
    "X_pool_nih = scaler.transform(nih_pool_feats_all)\n",
    "\n",
    "clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, C=1.0, random_state=SEED)\n",
    "clf.fit(X_train, chex_train_labels)\n",
    "\n",
    "\n",
    "def get_binary_logits(clf, X):\n",
    "    d = clf.decision_function(X)\n",
    "    return np.column_stack([-d, d])\n",
    "\n",
    "\n",
    "cal_logits = get_binary_logits(clf, X_cal)\n",
    "test_nih_logits = get_binary_logits(clf, X_test_nih)\n",
    "\n",
    "# DRE\n",
    "dre = AdaptiveDRE(n_components=4, weight_clip=20.0, random_state=SEED)\n",
    "dre.fit(chex_cal_feats, nih_pool_feats_all)\n",
    "cal_weights = dre.compute_weights(chex_cal_feats)\n",
    "test_nih_weights = dre.compute_weights(nih_test_feats)\n",
    "\n",
    "print(f\"CheXpert cal: {len(chex_cal_labels)} (prev={chex_cal_labels.mean():.3f})\")\n",
    "print(f\"NIH test:     {len(nih_test_labels)} (prev={nih_test_labels.mean():.3f})\")\n",
    "print(f\"NIH AUC: {roc_auc_score(nih_test_labels, clf.predict_proba(X_test_nih)[:, 1]):.4f}\")\n",
    "print(f\"DRE ESS: {dre.diagnostics(chex_cal_feats).ess_fraction:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Prevalence Estimation\n\n**BBSE** (Lipton et al., 2018): Estimate confusion matrix $C_{jk} = P(\\hat{y}=j \\mid y=k)$\non source data, get predicted proportions $\\hat{\\mu}$ on target, solve $C \\cdot p_{\\text{target}} = \\hat{\\mu}$.\nCan produce negative values when the classifier has high error rates and the shift is extreme.\n\n**MLLS** (Alexandari et al., 2020): EM algorithm that iteratively adjusts posteriors\nusing current prevalence estimate, then re-estimates prevalence from adjusted posteriors.\nNaturally produces valid probability distributions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def estimate_prevalence_bbse(clf, X_cal, y_cal, X_target_unlabeled):\n    \"\"\"BBSE: estimate p_target(y) from unlabeled target data.\"\"\"\n    y_pred_cal = clf.predict(X_cal)\n\n    # C[j,k] = P(y_hat=j | y=k) estimated from source calibration\n    C = np.zeros((2, 2))\n    for k in range(2):\n        mask = y_cal == k\n        for j in range(2):\n            C[j, k] = (y_pred_cal[mask] == j).mean()\n\n    # mu[j] = P_target(y_hat=j) from unlabeled target predictions\n    y_pred_target = clf.predict(X_target_unlabeled)\n    mu = np.array([(y_pred_target == j).mean() for j in range(2)])\n\n    # Solve C @ p_target = mu\n    p_target = np.linalg.solve(C, mu)\n    p_target = np.clip(p_target, 1e-4, 1 - 1e-4)\n    p_target = p_target / p_target.sum()\n\n    return p_target\n\n\ndef estimate_prevalence_mlls(clf, X_target_unlabeled, p_source, n_iter=200, tol=1e-8):\n    \"\"\"MLLS: Maximum Likelihood Label Shift via EM.\n\n    Iteratively:\n      E-step: adjust posteriors using current prevalence ratio\n      M-step: re-estimate target prevalence from adjusted posteriors\n    \"\"\"\n    probs = clf.predict_proba(X_target_unlabeled)  # [N, K]\n    w = np.ones(len(p_source))  # importance weights p_target/p_source\n\n    for _ in range(n_iter):\n        # E-step: adjust posteriors\n        adjusted = probs * w[np.newaxis, :]\n        adjusted = adjusted / adjusted.sum(axis=1, keepdims=True)\n\n        # M-step: estimate target prevalence\n        p_target = adjusted.mean(axis=0)\n        p_target = np.clip(p_target, 1e-8, 1.0)\n        p_target = p_target / p_target.sum()\n\n        w_new = p_target / p_source\n        if np.max(np.abs(w_new - w)) < tol:\n            break\n        w = w_new\n\n    return p_target\n\n\n# Estimate prevalences\np_source = np.array([1 - chex_cal_labels.mean(), chex_cal_labels.mean()])\np_target_oracle = np.array([1 - nih_test_labels.mean(), nih_test_labels.mean()])\np_target_bbse = estimate_prevalence_bbse(clf, X_cal, chex_cal_labels, X_pool_nih)\np_target_mlls = estimate_prevalence_mlls(clf, X_pool_nih, p_source)\n\nprint(f\"Source prevalence (CheXpert):  neg={p_source[0]:.4f}  pos={p_source[1]:.4f}\")\nprint(f\"BBSE estimate:                neg={p_target_bbse[0]:.4f}  pos={p_target_bbse[1]:.4f}\")\nprint(f\"MLLS estimate:                neg={p_target_mlls[0]:.4f}  pos={p_target_mlls[1]:.4f}\")\nprint(f\"Oracle (true NIH):            neg={p_target_oracle[0]:.4f}  pos={p_target_oracle[1]:.4f}\")\nprint()\nprint(f\"BBSE error:  |est - true| = {abs(p_target_bbse[1] - p_target_oracle[1]):.4f}\")\nprint(f\"MLLS error:  |est - true| = {abs(p_target_mlls[1] - p_target_oracle[1]):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Label-Shift Correction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_logits(logits, p_source, p_target):\n",
    "    \"\"\"Adjust logits for prevalence shift via Bayes' rule.\n",
    "\n",
    "    p_adjusted(y|x) = softmax(logit_y + log(p_target(y) / p_source(y)))\n",
    "    \"\"\"\n",
    "    log_ratio = np.log(p_target / p_source)\n",
    "    return logits + log_ratio[np.newaxis, :]\n",
    "\n",
    "\n",
    "class LabelShiftWCP:\n",
    "    \"\"\"Weighted CP with per-class label-shift importance weights.\n",
    "\n",
    "    Cal weight for sample i:       w_i = p_target(y_i) / p_source(y_i)\n",
    "    Test weight for candidate y:   w_test(y) = p_target(y) / p_source(y)\n",
    "\n",
    "    Since weights depend only on class (not features), each candidate class\n",
    "    gets a single constant quantile threshold — efficient for any test set size.\n",
    "\n",
    "    Optionally multiplies by DRE weights for joint covariate+label correction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, penalty=0.1, kreg=1, randomized=False):\n",
    "        self.score_fn = RAPS(penalty=penalty, kreg=kreg, randomized=randomized)\n",
    "        self.cal_scores_sorted = None\n",
    "        self.cal_weights_sorted = None\n",
    "\n",
    "    def calibrate(self, logits, labels, p_source, p_target, dre_weights=None):\n",
    "        \"\"\"Calibrate with label-shift (and optionally DRE) weights.\"\"\"\n",
    "        logits_t = torch.tensor(logits, dtype=torch.float32)\n",
    "        labels_t = torch.tensor(labels, dtype=torch.long)\n",
    "        scores = self.score_fn(logits_t, labels_t).numpy()\n",
    "\n",
    "        # Label-shift weights per calibration sample\n",
    "        weights = p_target[labels] / p_source[labels]\n",
    "        if dre_weights is not None:\n",
    "            weights = weights * dre_weights\n",
    "\n",
    "        sort_idx = np.argsort(scores)\n",
    "        self.cal_scores_sorted = scores[sort_idx]\n",
    "        self.cal_weights_sorted = weights[sort_idx]\n",
    "\n",
    "    def predict(self, logits, p_source, p_target, alpha=0.1, dre_test_weights=None):\n",
    "        \"\"\"Predict with per-class test weights.\"\"\"\n",
    "        logits_t = torch.tensor(logits, dtype=torch.float32)\n",
    "        all_scores = self.score_fn(logits_t).numpy()\n",
    "\n",
    "        N, K = all_scores.shape\n",
    "        n_cal = len(self.cal_scores_sorted)\n",
    "        prediction_sets = np.zeros((N, K), dtype=np.int32)\n",
    "\n",
    "        for c in range(K):\n",
    "            ls_weight = p_target[c] / p_source[c]\n",
    "\n",
    "            if dre_test_weights is not None:\n",
    "                # Per-test-point weights: DRE * label-shift\n",
    "                test_w = dre_test_weights * ls_weight\n",
    "                cal_w = self.cal_weights_sorted[np.newaxis, :]  # [1, n_cal]\n",
    "                all_w = np.concatenate(\n",
    "                    [np.broadcast_to(cal_w, (N, n_cal)), test_w[:, np.newaxis]], axis=1\n",
    "                )\n",
    "                p_norm = all_w / all_w.sum(axis=1, keepdims=True)\n",
    "                cumprob = np.cumsum(p_norm[:, :n_cal], axis=1)\n",
    "\n",
    "                reached = cumprob >= (1 - alpha)\n",
    "                has_any = reached.any(axis=1)\n",
    "                first_idx = np.argmax(reached, axis=1)\n",
    "                q_hat = np.where(\n",
    "                    has_any, self.cal_scores_sorted[first_idx], np.inf\n",
    "                )\n",
    "            else:\n",
    "                # Constant test weight -> single quantile per class\n",
    "                all_w = np.append(self.cal_weights_sorted, ls_weight)\n",
    "                p_norm = all_w / all_w.sum()\n",
    "                cumprob = np.cumsum(p_norm[:n_cal])\n",
    "\n",
    "                reached = cumprob >= (1 - alpha)\n",
    "                q_hat_c = (\n",
    "                    float(self.cal_scores_sorted[np.argmax(reached)])\n",
    "                    if reached.any()\n",
    "                    else float(\"inf\")\n",
    "                )\n",
    "                q_hat = q_hat_c\n",
    "\n",
    "            prediction_sets[:, c] = (all_scores[:, c] <= q_hat).astype(np.int32)\n",
    "\n",
    "        return prediction_sets\n",
    "\n",
    "\n",
    "print(\"Methods defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate on Target Pathology\n",
    "\n",
    "Compare 5 methods:\n",
    "1. **Standard CP** — no shift correction (baseline)\n",
    "2. **WCP** — DRE covariate-shift correction only (baseline)\n",
    "3. **Prior-adjusted CP** — logit adjustment for label shift\n",
    "4. **Label-shift WCP** — weighted quantile with label-shift weights\n",
    "5. **Prior-adj + DRE WCP** — logit adjustment + DRE weights (both shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "alphas = np.linspace(0.01, 0.5, 50)\n\n# Use MLLS estimate (more robust than BBSE)\np_est = p_target_mlls\n\n# Pre-compute adjusted logits\nadj_cal_logits = adjust_logits(cal_logits, p_source, p_est)\nadj_test_logits = adjust_logits(test_nih_logits, p_source, p_est)\n\n# Also with oracle for comparison\nadj_cal_logits_oracle = adjust_logits(cal_logits, p_source, p_target_oracle)\nadj_test_logits_oracle = adjust_logits(test_nih_logits, p_source, p_target_oracle)\n\n\ndef run_evaluation(alphas):\n    \"\"\"Run all methods across alpha range.\"\"\"\n    results = {}\n\n    for alpha in alphas:\n        pred_sets = {}\n\n        # 1. Standard CP\n        cp = ConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n        cp.calibrate(cal_logits, chex_cal_labels, alpha=alpha)\n        pred_sets[\"Standard CP\"] = cp.predict(test_nih_logits)\n\n        # 2. WCP (DRE only)\n        wcp = WeightedConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n        wcp.calibrate(cal_logits, chex_cal_labels, cal_weights)\n        pred_sets[\"WCP (DRE)\"] = wcp.predict(test_nih_logits, test_nih_weights, alpha=alpha)\n\n        # 3. Prior-adjusted CP (MLLS)\n        pa_cp = ConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n        pa_cp.calibrate(adj_cal_logits, chex_cal_labels, alpha=alpha)\n        pred_sets[\"Prior-adj CP\"] = pa_cp.predict(adj_test_logits)\n\n        # 4. Label-shift WCP (MLLS)\n        ls_wcp = LabelShiftWCP(penalty=0.1, kreg=1, randomized=False)\n        ls_wcp.calibrate(cal_logits, chex_cal_labels, p_source, p_est)\n        pred_sets[\"LS-WCP\"] = ls_wcp.predict(\n            test_nih_logits, p_source, p_est, alpha=alpha\n        )\n\n        # 5. Prior-adjusted + DRE WCP (MLLS)\n        pa_dre = WeightedConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n        pa_dre.calibrate(adj_cal_logits, chex_cal_labels, cal_weights)\n        pred_sets[\"Prior-adj+DRE\"] = pa_dre.predict(\n            adj_test_logits, test_nih_weights, alpha=alpha\n        )\n\n        # 6. Prior-adjusted CP (Oracle) — upper bound\n        pa_oracle = ConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n        pa_oracle.calibrate(adj_cal_logits_oracle, chex_cal_labels, alpha=alpha)\n        pred_sets[\"PA-CP (oracle)\"] = pa_oracle.predict(adj_test_logits_oracle)\n\n        for name, ps in pred_sets.items():\n            cov = compute_coverage(ps, nih_test_labels)\n            preds, defer_mask = _predictions_from_sets(ps, test_nih_logits)\n            sys = compute_system_accuracy(\n                preds, nih_test_labels, defer_mask, expert_accuracy=EXPERT_ACCURACY\n            )\n            results.setdefault(name, []).append(\n                DeferralResult(\n                    method=name,\n                    alpha_or_threshold=float(alpha),\n                    system_accuracy=sys[\"system_accuracy\"],\n                    deferral_rate=sys[\"deferral_rate\"],\n                    coverage_rate=cov[\"coverage_rate\"],\n                    average_set_size=cov[\"average_set_size\"],\n                    model_accuracy_on_kept=sys[\"model_accuracy_on_kept\"],\n                    n_total=len(nih_test_labels),\n                    n_deferred=sys[\"n_deferred\"],\n                )\n            )\n\n    return results\n\n\nall_results = run_evaluation(alphas)\nprint(\"All methods evaluated.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary at alpha = 0.1\n",
    "alpha_target = 0.1\n",
    "rows = []\n",
    "for name, res_list in all_results.items():\n",
    "    r = min(res_list, key=lambda r: abs(r.alpha_or_threshold - alpha_target))\n",
    "    rows.append(\n",
    "        {\n",
    "            \"Method\": name,\n",
    "            \"Coverage\": f\"{r.coverage_rate:.4f}\",\n",
    "            \"Avg |C|\": f\"{r.average_set_size:.3f}\",\n",
    "            \"Deferral\": f\"{r.deferral_rate:.4f}\",\n",
    "            \"Sys Acc\": f\"{r.system_accuracy:.4f}\",\n",
    "            \"Model Acc\": f\"{r.model_accuracy_on_kept:.4f}\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"\\n{TARGET_PATHOLOGY} at alpha={alpha_target} (target coverage >= 90%)\")\n",
    "print(\"=\" * 85)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Class-Conditional Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "alpha_target = 0.1\n\n\ndef class_conditional_analysis(pred_sets, labels, method_name):\n    set_sizes = pred_sets.sum(axis=1)\n    covered = pred_sets[np.arange(len(labels)), labels].astype(bool)\n    deferred = set_sizes != 1\n    rows = []\n    for c, cname in [(0, \"Neg\"), (1, \"Pos\")]:\n        m = labels == c\n        rows.append(\n            {\n                \"Method\": method_name,\n                \"Class\": cname,\n                \"N\": int(m.sum()),\n                \"Cov\": f\"{covered[m].mean():.4f}\",\n                \"Defer\": f\"{deferred[m].mean():.4f}\",\n                \"|C|\": f\"{set_sizes[m].mean():.3f}\",\n            }\n        )\n    rows.append(\n        {\n            \"Method\": method_name,\n            \"Class\": \"All\",\n            \"N\": len(labels),\n            \"Cov\": f\"{covered.mean():.4f}\",\n            \"Defer\": f\"{deferred.mean():.4f}\",\n            \"|C|\": f\"{set_sizes.mean():.3f}\",\n        }\n    )\n    return rows\n\n\nall_rows = []\nmethod_configs = [\n    (\"Standard CP\", cal_logits, test_nih_logits, False, False),\n    (\"WCP (DRE)\", cal_logits, test_nih_logits, True, False),\n    (\"Prior-adj CP\", adj_cal_logits, adj_test_logits, False, False),\n    (\"LS-WCP\", cal_logits, test_nih_logits, False, True),\n    (\"Prior-adj+DRE\", adj_cal_logits, adj_test_logits, True, False),\n    (\"PA-CP (oracle)\", adj_cal_logits_oracle, adj_test_logits_oracle, False, False),\n]\n\nfor name, c_lg, t_lg, use_dre, use_ls in method_configs:\n    if use_ls:\n        pred = LabelShiftWCP(penalty=0.1, kreg=1, randomized=False)\n        pred.calibrate(c_lg, chex_cal_labels, p_source, p_est)\n        ps = pred.predict(t_lg, p_source, p_est, alpha=alpha_target)\n    elif use_dre:\n        pred = WeightedConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n        pred.calibrate(c_lg, chex_cal_labels, cal_weights)\n        ps = pred.predict(t_lg, test_nih_weights, alpha=alpha_target)\n    else:\n        pred = ConformalPredictor(penalty=0.1, kreg=1, randomized=False)\n        pred.calibrate(c_lg, chex_cal_labels, alpha=alpha_target)\n        ps = pred.predict(t_lg)\n    all_rows.extend(class_conditional_analysis(ps, nih_test_labels, name))\n\nprint(f\"Class-conditional analysis for {TARGET_PATHOLOGY} at alpha={alpha_target}\")\nprint(\"=\" * 75)\nprint(pd.DataFrame(all_rows).to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Coverage and Deferral vs Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ncolors = {\n    \"Standard CP\": \"#1f77b4\",\n    \"WCP (DRE)\": \"#ff7f0e\",\n    \"Prior-adj CP\": \"#2ca02c\",\n    \"LS-WCP\": \"#d62728\",\n    \"Prior-adj+DRE\": \"#9467bd\",\n    \"PA-CP (oracle)\": \"#8c564b\",\n}\n\n# Coverage vs alpha\nax = axes[0]\nfor name, res in all_results.items():\n    a = [r.alpha_or_threshold for r in res]\n    c = [r.coverage_rate for r in res]\n    ls = \"--\" if \"oracle\" in name else \"-\"\n    ax.plot(a, c, label=name, color=colors[name], linewidth=1.5, linestyle=ls, marker=\"o\", markersize=2)\nax.plot(alphas, 1 - alphas, \"k--\", alpha=0.5, linewidth=1.5, label=r\"Ideal $1-\\alpha$\")\nax.set_xlabel(r\"$\\alpha$\")\nax.set_ylabel(\"Coverage\")\nax.set_title(f\"Coverage vs Alpha ({TARGET_PATHOLOGY})\")\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\n\n# Deferral vs alpha\nax = axes[1]\nfor name, res in all_results.items():\n    a = [r.alpha_or_threshold for r in res]\n    d = [r.deferral_rate for r in res]\n    ls = \"--\" if \"oracle\" in name else \"-\"\n    ax.plot(a, d, label=name, color=colors[name], linewidth=1.5, linestyle=ls, marker=\"o\", markersize=2)\nax.set_xlabel(r\"$\\alpha$\")\nax.set_ylabel(\"Deferral Rate\")\nax.set_title(f\"Deferral Rate vs Alpha ({TARGET_PATHOLOGY})\")\nax.legend(fontsize=8)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Pathology Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "alpha_target = 0.1\nmulti_results = []\n\nfor pathology in COMMON_PATHOLOGIES:\n    # Binary labels\n    c_feats, c_labels, _ = extract_binary_labels(\n        chexpert.features, chexpert.labels, COMMON_PATHOLOGIES, pathology\n    )\n    n_feats, n_labels, _ = extract_binary_labels(\n        nih.features, nih.labels, COMMON_PATHOLOGIES, pathology\n    )\n\n    # Splits\n    c_tr_f, c_tmp_f, c_tr_l, c_tmp_l = train_test_split(\n        c_feats, c_labels, test_size=0.4, random_state=SEED, stratify=c_labels\n    )\n    c_cal_f, _, c_cal_l, _ = train_test_split(\n        c_tmp_f, c_tmp_l, test_size=0.5, random_state=SEED, stratify=c_tmp_l\n    )\n    _, n_te_f, _, n_te_l = train_test_split(\n        n_feats, n_labels, test_size=0.5, random_state=SEED, stratify=n_labels\n    )\n\n    # Classifier\n    sc = StandardScaler()\n    Xtr = sc.fit_transform(c_tr_f)\n    Xcal = sc.transform(c_cal_f)\n    Xte = sc.transform(n_te_f)\n    Xpool = sc.transform(nih_pool_feats_all)\n\n    model = LogisticRegression(solver=\"lbfgs\", max_iter=1000, C=1.0, random_state=SEED)\n    model.fit(Xtr, c_tr_l)\n    nih_auc = roc_auc_score(n_te_l, model.predict_proba(Xte)[:, 1])\n\n    def _logits(m, X):\n        d = m.decision_function(X)\n        return np.column_stack([-d, d])\n\n    c_lg = _logits(model, Xcal)\n    t_lg = _logits(model, Xte)\n\n    # DRE\n    d = AdaptiveDRE(n_components=4, weight_clip=20.0, random_state=SEED)\n    d.fit(c_cal_f, nih_pool_feats_all)\n    cw = d.compute_weights(c_cal_f)\n    tw = d.compute_weights(n_te_f)\n\n    # Prevalence estimation\n    p_src = np.array([1 - c_cal_l.mean(), c_cal_l.mean()])\n    p_tgt_mlls = estimate_prevalence_mlls(model, Xpool, p_src)\n    p_tgt_oracle = np.array([1 - n_te_l.mean(), n_te_l.mean()])\n\n    # Adjusted logits (MLLS)\n    adj_c_lg = adjust_logits(c_lg, p_src, p_tgt_mlls)\n    adj_t_lg = adjust_logits(t_lg, p_src, p_tgt_mlls)\n\n    # Adjusted logits (Oracle)\n    adj_c_lg_o = adjust_logits(c_lg, p_src, p_tgt_oracle)\n    adj_t_lg_o = adjust_logits(t_lg, p_src, p_tgt_oracle)\n\n    row = {\n        \"Pathology\": pathology,\n        \"AUC\": f\"{nih_auc:.3f}\",\n        \"Src prev\": f\"{p_src[1]:.3f}\",\n        \"Tgt prev\": f\"{p_tgt_oracle[1]:.3f}\",\n        \"MLLS prev\": f\"{p_tgt_mlls[1]:.3f}\",\n    }\n\n    # Evaluate methods\n    method_configs = [\n        (\"Std\", ConformalPredictor, c_lg, t_lg, None, None, None, None),\n        (\"WCP\", WeightedConformalPredictor, c_lg, t_lg, cw, tw, None, None),\n        (\"PA-CP\", ConformalPredictor, adj_c_lg, adj_t_lg, None, None, None, None),\n        (\"LS\", \"ls_wcp\", c_lg, t_lg, None, None, p_src, p_tgt_mlls),\n        (\"PA+DRE\", WeightedConformalPredictor, adj_c_lg, adj_t_lg, cw, tw, None, None),\n        (\"PA(orc)\", ConformalPredictor, adj_c_lg_o, adj_t_lg_o, None, None, None, None),\n    ]\n\n    for mname, mclass, cl, tl, cw_, tw_, ps, pt in method_configs:\n        if mclass == \"ls_wcp\":\n            pred = LabelShiftWCP(penalty=0.1, kreg=1, randomized=False)\n            pred.calibrate(cl, c_cal_l, ps, pt)\n            ps_out = pred.predict(tl, ps, pt, alpha=alpha_target)\n        elif mclass == WeightedConformalPredictor:\n            pred = mclass(penalty=0.1, kreg=1, randomized=False)\n            pred.calibrate(cl, c_cal_l, cw_)\n            ps_out = pred.predict(tl, tw_, alpha=alpha_target)\n        else:\n            pred = mclass(penalty=0.1, kreg=1, randomized=False)\n            pred.calibrate(cl, c_cal_l, alpha=alpha_target)\n            ps_out = pred.predict(tl)\n\n        cov = ps_out[np.arange(len(n_te_l)), n_te_l].mean()\n        defer = (ps_out.sum(axis=1) != 1).mean()\n        row[f\"{mname} Cov\"] = f\"{cov:.3f}\"\n        row[f\"{mname} Def\"] = f\"{defer:.3f}\"\n\n    multi_results.append(row)\n    print(\n        f\"{pathology:<16} MLLS={p_tgt_mlls[1]:.3f}(true={p_tgt_oracle[1]:.3f})  \"\n        f\"Std={row['Std Def']}  WCP={row['WCP Def']}  \"\n        f\"PA-CP={row['PA-CP Cov']}/{row['PA-CP Def']}  \"\n        f\"PA+DRE={row['PA+DRE Cov']}/{row['PA+DRE Def']}  \"\n        f\"PA(orc)={row['PA(orc) Cov']}/{row['PA(orc) Def']}\"\n    )\n\ndf_multi = pd.DataFrame(multi_results)\nprint(f\"\\n{'=' * 150}\")\nprint(f\"Multi-pathology comparison at alpha={alpha_target}\")\nprint(f\"{'=' * 150}\")\nprint(df_multi.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Multi-pathology visualization\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\npathology_names = [r[\"Pathology\"] for r in multi_results]\nx = np.arange(len(pathology_names))\nwidth = 0.13\nmethod_keys = [\n    (\"Std\", \"Standard CP\"),\n    (\"WCP\", \"WCP (DRE)\"),\n    (\"PA-CP\", \"Prior-adj CP\"),\n    (\"LS\", \"LS-WCP\"),\n    (\"PA+DRE\", \"Prior-adj+DRE\"),\n    (\"PA(orc)\", \"PA-CP (oracle)\"),\n]\n\n# Deferral rates\nax = axes[0]\nfor i, (key, label) in enumerate(method_keys):\n    vals = [float(r[f\"{key} Def\"]) for r in multi_results]\n    ax.bar(x + i * width, vals, width, label=label)\nax.set_xticks(x + 2.5 * width)\nax.set_xticklabels(pathology_names, rotation=45, ha=\"right\")\nax.set_ylabel(\"Deferral Rate\")\nax.set_title(f\"Deferral Rate by Pathology (alpha={alpha_target})\")\nax.legend(fontsize=7, loc=\"upper left\")\nax.grid(True, alpha=0.3, axis=\"y\")\n\n# Coverage\nax = axes[1]\nfor i, (key, label) in enumerate(method_keys):\n    vals = [float(r[f\"{key} Cov\"]) for r in multi_results]\n    ax.bar(x + i * width, vals, width, label=label)\nax.axhline(y=0.9, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Target (90%)\")\nax.set_xticks(x + 2.5 * width)\nax.set_xticklabels(pathology_names, rotation=45, ha=\"right\")\nax.set_ylabel(\"Coverage\")\nax.set_title(f\"Coverage by Pathology (alpha={alpha_target})\")\nax.legend(fontsize=7, loc=\"lower left\")\nax.grid(True, alpha=0.3, axis=\"y\")\nax.set_ylim(0.5, 1.05)\n\nplt.tight_layout()\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wcp-l2d",
   "language": "python",
   "name": "wcp-l2d"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}